{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRemgA0agckAzYdo7JzdHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsingh9076/Natural_Language_Processing/blob/master/RNN/RNN_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btR67XQS-9Fv",
        "outputId": "788b5b82-f595-4223-aebc-88475a8db8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 7.023956792935076\n",
            "Epoch 100, Loss: 15.45552191295862\n",
            "Epoch 200, Loss: 17.414384196189374\n",
            "Epoch 300, Loss: 18.020117181435406\n",
            "Epoch 400, Loss: 18.238020568687755\n",
            "Epoch 500, Loss: 18.321933207395478\n",
            "Epoch 600, Loss: 18.355230814747525\n",
            "Epoch 700, Loss: 18.36861875390664\n",
            "Epoch 800, Loss: 18.374033122327575\n",
            "Epoch 900, Loss: 18.37622856330542\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleCharRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
        "        self.Why = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output\n",
        "        self.bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "        self.by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "        # Vocabulary\n",
        "        self.char_to_ix = {}\n",
        "        self.ix_to_char = {}\n",
        "\n",
        "    def build_vocab(self, text):\n",
        "        chars = sorted(set(text))\n",
        "        self.char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, inputs, h_prev):\n",
        "        # Update hidden state\n",
        "        self.h_next = np.tanh(np.dot(self.Wxh, inputs) + np.dot(self.Whh, h_prev) + self.bh)\n",
        "\n",
        "        # Compute output\n",
        "        output = np.dot(self.Why, self.h_next) + self.by\n",
        "        output_probs = self.sigmoid(output)\n",
        "\n",
        "        return output_probs, self.h_next\n",
        "\n",
        "    def backward(self, inputs, targets, h_prev, output_probs, learning_rate=0.01):\n",
        "        # Compute gradients\n",
        "        dWhy = np.dot((output_probs - targets), self.h_next.T)\n",
        "        dby = output_probs - targets\n",
        "        dh_next = np.dot(self.Why.T, dby) + np.dot(self.Whh.T, (1 - self.h_next**2) * h_prev)\n",
        "        dWhh = np.dot((1 - self.h_next**2) * h_prev, h_prev.T)\n",
        "        dWxh = np.dot((1 - self.h_next**2) * h_prev, inputs.T)\n",
        "        dbh = (1 - self.h_next**2) * h_prev\n",
        "\n",
        "        # Update parameters\n",
        "        self.Why -= learning_rate * dWhy\n",
        "        self.by -= learning_rate * dby\n",
        "        self.Whh -= learning_rate * dWhh\n",
        "        self.Wxh -= learning_rate * dWxh\n",
        "        self.bh -= learning_rate * dbh\n",
        "\n",
        "    def train(self, text, learning_rate=0.01, epochs=1000):\n",
        "        self.build_vocab(text)\n",
        "        self.output_size = len(self.char_to_ix)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            h_prev = np.zeros((self.hidden_size, 1))  # Initial hidden state\n",
        "            loss = 0\n",
        "\n",
        "            for t in range(len(text)-1):\n",
        "                # Forward pass\n",
        "                x = np.zeros((self.input_size, 1))\n",
        "                x[self.char_to_ix[text[t]]] = 1  # One-hot encoding for the character\n",
        "                y, h_prev = self.forward(x, h_prev)\n",
        "\n",
        "                # Compute loss\n",
        "                target = np.zeros((self.output_size, 1))\n",
        "                target[self.char_to_ix[text[t+1]]] = 1  # One-hot encoding for the next character\n",
        "                loss += -np.sum(target * np.log(y))\n",
        "\n",
        "                # Backpropagation\n",
        "                self.backward(x, target, h_prev, y, learning_rate)\n",
        "\n",
        "            # Print loss for monitoring training progress\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "# Example usage\n",
        "text_data = \"hello world\"\n",
        "input_size = len(set(text_data))\n",
        "hidden_size = 10\n",
        "output_size = len(set(text_data))\n",
        "\n",
        "char_rnn = SimpleCharRNN(input_size, hidden_size, output_size)\n",
        "char_rnn.train(text_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleCharRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
        "        self.Why = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output\n",
        "        self.bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "        self.by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "        # Vocabulary\n",
        "        self.char_to_ix = {}\n",
        "        self.ix_to_char = {}\n",
        "\n",
        "    def build_vocab(self, text):\n",
        "        chars = sorted(set(text))\n",
        "        self.char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, inputs, h_prev):\n",
        "        # Update hidden state\n",
        "        self.h_next = np.tanh(np.dot(self.Wxh, inputs) + np.dot(self.Whh, h_prev) + self.bh)\n",
        "\n",
        "        # Compute output\n",
        "        output = np.dot(self.Why, self.h_next) + self.by\n",
        "        output_probs = self.sigmoid(output)\n",
        "\n",
        "        return output_probs, self.h_next\n",
        "\n",
        "    def backward(self, inputs, targets, h_prev, output_probs, learning_rate=0.01):\n",
        "        # Compute gradients\n",
        "        dWhy = np.dot((output_probs - targets), self.h_next.T)\n",
        "        dby = output_probs - targets\n",
        "        dh_next = np.dot(self.Why.T, dby) + np.dot(self.Whh.T, (1 - self.h_next**2) * h_prev)\n",
        "        dWhh = np.dot((1 - self.h_next**2) * h_prev, h_prev.T)\n",
        "        dWxh = np.dot((1 - self.h_next**2) * h_prev, inputs.T)\n",
        "        dbh = (1 - self.h_next**2) * h_prev\n",
        "\n",
        "        # Update parameters\n",
        "        self.Why -= learning_rate * dWhy\n",
        "        self.by -= learning_rate * dby\n",
        "        self.Whh -= learning_rate * dWhh\n",
        "        self.Wxh -= learning_rate * dWxh\n",
        "        self.bh -= learning_rate * dbh\n",
        "\n",
        "    def train(self, text, learning_rate=0.01, epochs=1000):\n",
        "        self.build_vocab(text)\n",
        "        self.output_size = len(self.char_to_ix)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            h_prev = np.zeros((self.hidden_size, 1))  # Initial hidden state\n",
        "            loss = 0\n",
        "\n",
        "            for t in range(len(text)-1):\n",
        "                # Forward pass\n",
        "                x = np.zeros((self.input_size, 1))\n",
        "                x[self.char_to_ix[text[t]]] = 1  # One-hot encoding for the character\n",
        "                y, h_prev = self.forward(x, h_prev)\n",
        "\n",
        "                # Compute loss\n",
        "                target = np.zeros((self.output_size, 1))\n",
        "                target[self.char_to_ix[text[t+1]]] = 1  # One-hot encoding for the next character\n",
        "                loss += -np.sum(target * np.log(y))\n",
        "\n",
        "                # Backpropagation\n",
        "                self.backward(x, target, h_prev, y, learning_rate)\n",
        "\n",
        "            # Print loss for monitoring training progress\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "# Example usage\n",
        "text_data = \"hello world\"\n",
        "input_size = len(set(text_data))\n",
        "hidden_size = 10\n",
        "output_size = len(set(text_data))\n",
        "\n",
        "char_rnn = SimpleCharRNN(input_size, hidden_size, output_size)\n",
        "char_rnn.train(text_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlnOMcgx_yc5",
        "outputId": "b52f1c3e-c61f-4a51-e9bc-40396fb11063"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 7.023458664808913\n",
            "Epoch 100, Loss: 15.45564375087393\n",
            "Epoch 200, Loss: 17.41438094076354\n",
            "Epoch 300, Loss: 18.020105598891824\n",
            "Epoch 400, Loss: 18.23801442421987\n",
            "Epoch 500, Loss: 18.321930607379507\n",
            "Epoch 600, Loss: 18.355229797461504\n",
            "Epoch 700, Loss: 18.36861836939374\n",
            "Epoch 800, Loss: 18.374032979475018\n",
            "Epoch 900, Loss: 18.37622851073108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5T7u3a7AuHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}